{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6571db27",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer, WordNetLemmatizer\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyLDAvis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgensim_models\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgensimvis\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyLDAvis\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import corpora\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import  sys\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3187da4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add parent directory to path to import modules from src\n",
    "rpath = os.path.abspath(r'C:\\Users\\Eshetu\\Desktop\\10Academy\\week0_starter_network_analysis')\n",
    "if rpath not in sys.path:\n",
    "    sys.path.insert(0, rpath)\n",
    "\n",
    "from src.loader import SlackDataLoader\n",
    "import src.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e377ff7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SlackDataLoader' object has no attribute 'get_ussers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m SlackDataLoader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/Eshetu/Desktop/10Academy/week0_starter_network_analysis/anonymized/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_channel_messages\u001b[39m(channel):\n\u001b[0;32m      3\u001b[0m     channel_messages \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_messages_on_channel(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/Eshetu/Desktop/10Academy/week0_starter_network_analysis/anonymized/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchannel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n",
      "File \u001b[1;32m~\\Desktop\\10Academy\\week0_starter_network_analysis\\src\\loader.py:37\u001b[0m, in \u001b[0;36mSlackDataLoader.__init__\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m path\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_channels()\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_ussers()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SlackDataLoader' object has no attribute 'get_ussers'"
     ]
    }
   ],
   "source": [
    "data_loader = SlackDataLoader(\"C:/Users/Eshetu/Desktop/10Academy/week0_starter_network_analysis/anonymized/\")\n",
    "def get_channel_messages(channel):\n",
    "    channel_messages = utils.get_messages_on_channel(f\"C:/Users/Eshetu/Desktop/10Academy/week0_starter_network_analysis/anonymized/{channel}\") \n",
    "    # Create an empty DataFrame\n",
    "    df = pd.DataFrame(channel_messages)\n",
    "    return df\n",
    "\n",
    "def get_all_channels_message():\n",
    "    dfs = []  # List to store individual DataFrames\n",
    "\n",
    "    for channel in data_loader.channels:\n",
    "        dfs.append(get_channel_messages(channel[\"name\"]))\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    result_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c1bde93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Extract and remove URLs\n",
    "    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "    for url in urls:\n",
    "        text = text.replace(url, '')\n",
    "\n",
    "    text = re.sub(r'<@.*?>', '', text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Perform stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    # Perform lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Join the tokens back into a string\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6032594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "    sentence_list = [tweet for tweet in df['cleaned_text']]\n",
    "    word_list = [sent.split() for sent in sentence_list]\n",
    "\n",
    "    #Create dictionary which contains Id and word\n",
    "    word_to_id = corpora.Dictionary(word_list) #generate unique tokens\n",
    "    corpus = [word_to_id.doc2bow(tweet) for tweet in word_list]\n",
    "    \n",
    "    return df, word_list, word_to_id, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "903b172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(corpus, word_to_id):\n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus,\n",
    "                                        id2word=word_to_id,\n",
    "                                        num_topics=5,\n",
    "                                        random_state=100,\n",
    "                                        update_every=1,\n",
    "                                        chunksize=100,\n",
    "                                        passes=10,\n",
    "                                        alpha='auto',\n",
    "                                        per_word_topics=True)    \n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "264cb46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_topics(lda_model):\n",
    "    pprint(lda_model.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "985628b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_analysis(lda_model, corpus, word_list, word_to_id):\n",
    "    print('\\nPerplexity: ', lda_model.log_perplexity(corpus))\n",
    "    doc_lda = lda_model[corpus]\n",
    "\n",
    "\n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=word_list, dictionary=word_to_id, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('\\n Lda model Coherence Score/Accuracy on Tweets: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bbeab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_topics(df):\n",
    "    df, word_list, word_to_id, corpus = prepare_data(df)\n",
    "    lda_model = build_model(corpus, word_to_id)\n",
    "\n",
    "    # Show the top 10 topics\n",
    "    show_topics(lda_model)\n",
    "    \n",
    "\n",
    "    # Visualize the top 10 topics\n",
    "    pyLDAvis.enable_notebook()\n",
    "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, word_to_id)\n",
    "    return LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce281a56",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'src.utils' has no attribute 'get_messages_on_channel'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m get_channel_messages(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall-week1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m get_top_topics(df)\n",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m, in \u001b[0;36mget_channel_messages\u001b[1;34m(channel)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_channel_messages\u001b[39m(channel):\n\u001b[1;32m----> 2\u001b[0m     channel_messages \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_messages_on_channel(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/Eshetu/Desktop/10Academy/week0_starter_network_analysis/anonymized/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchannel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Create an empty DataFrame\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(channel_messages)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'src.utils' has no attribute 'get_messages_on_channel'"
     ]
    }
   ],
   "source": [
    "df = get_channel_messages(\"all-week1\")\n",
    "get_top_topics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0147315",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
